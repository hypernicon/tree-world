{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acada8bb-6459-4511-8239-327c9a70b19b",
   "metadata": {},
   "source": [
    "# Map-based Memory Validation\n",
    "\n",
    "The idea of a location map is to remember sensory data. For our simple navigator, sensory data is like smell: our trees distributed through the environment diffuse scents indicating their identity. At a given point, the sense of smell detects a combination of data from nearby trees.\n",
    "\n",
    "## A simple chemical sensor\n",
    "\n",
    "The 'scent' $z$ of a tree should be detected proprotionately to the distance from a tree. I model the decay of the scent via a diffusion process. The model for diffusion (the physical process, not the algorithm) is based on the heat equation $u(x,t)$ is $$\\frac{1}{2} \\frac{\\partial u}{\\partial t} = \\sum_i \\frac{\\partial^2 u}{\\partial x_i^2} = \\Delta u,$$ where $\\Delta u$ is called the _Laplacian_. The heat equation is solved by\n",
    "$$u(x, t) = \\frac{c}{\\sqrt{t^d}} \\exp\\left(-\\frac{1}{2t} \\|x - \\mu\\|^2\\right),$$ \n",
    "where $d$ is the dimension of the space, $c$ is any constant, and $\\mu$ is the intial point where diffusion starts.\n",
    "\n",
    "Here $u(x,t)$ is the diffused quantity. For clarity, $x$ is the location of the sensor, and $\\mu$ is the location of the source tree. At $u(x, 0) = c$, this is a point mass at $\\mu$. Our interest is to choose a virtual time $t$ such that controls the spread of the tree's sense such that (a) trees can be sensed from a distance, and (b) trees are still distinct [note that $u(x, \\infty) = 0$]. For simplicity, we set $c = \\sqrt{t^d}$ so that $u(\\mu, t) = 1$ for all $t$; that is, the sensor outputs the vector with all ones at the source. \n",
    "\n",
    "To choose $t$, we set a maximum distance $M$ and a target sensory value $a$ such that the diffused scent has magnitude $a$ at distance $M$. That is, $u(x, t_M) = a$ when $\\|x - \\mu\\| = M$, which reduces to $$u(x, t) = \\exp\\left(-\\frac{M^2}{2t}\\right) = a \\quad\\quad\\implies\\quad\\quad t_M = \\frac{M^2}{2 \\log({1}/{a})}.$$\n",
    "\n",
    "Finally, given a set of trees at locations $\\{\\ell_i\\}$ with scent embeddings $z_i$, a sensor at position $x$ will read the sum of the diffused scents\n",
    "$$\n",
    "z = \\sum_i z_i \\exp\\left(-\\frac{1}{2t_M} \\| x - \\ell_i\\|^2\\right) = KZ\n",
    "$$\n",
    "for the kernel matrix $K_ij = u_{i}(x_j, t_M)$ and scent matrix $Z$ with $z_i$ as the $i^{th}$ row.\n",
    "\n",
    "This sensor is implemented by the following function, with some wrapping to handle batch sizes and the possibility of multiple sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5408e2-cb43-4802-9f09-554791b60fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sense(self, source_locations: torch.Tensor, source_embeddings: torch.Tensor, sensor_positions: torch.Tensor, \n",
    "          M: float, a: float=0.1) -> torch.Tensor:\n",
    "    # we expect source_locations to be a >2D tensor of shape (..., num_sources, dim)\n",
    "    # we expect sensor_positions to be a >2D tensor of shape (..., num_sensors, dim) OR a 1D tensor of shape (..., dim)\n",
    "    # we expect source_embeddings to be a >2D tensor of shape (..., num_sources, embed_dim)\n",
    "\n",
    "    # special case for a single sensor\n",
    "    single_sensor = sensor_positions.ndim < source_locations.ndim\n",
    "    if single_sensor:\n",
    "        sensor_positions = sensor_positions[..., None, :]\n",
    "    \n",
    "    assert source_locations.ndim == sensor_positions.ndim == source_embeddings.ndim\n",
    "   \n",
    "    # compute the virtual time\n",
    "    t_M = M**2 / 2 / math.log(1/a)\n",
    "\n",
    "    # compute the distance between the sensor and the source trees\n",
    "    # This will yield a tensor of shape (..., num_sensors, num_sources)\n",
    "    distances = torch.cdist(sensor_positions, source_locations)\n",
    "\n",
    "    # now compute the kernel for each sensor-source pair as shape (..., num_sensors, num_sources)\n",
    "    kernel = torch.exp(-0.5 * (distances).pow(2) / t_M)\n",
    "\n",
    "    # now compute the embedding for each sensor as shape (..., num_sensors, embed_dim)\n",
    "    if kernel.ndim == 2:\n",
    "        embedding = torch.mm(kernel, source_embeddings)\n",
    "    else:\n",
    "        embedding = torch.bmm(kernel, source_embeddings)\n",
    "\n",
    "    if single_sensor:\n",
    "        embedding = embedding.squeeze(-2)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686e639",
   "metadata": {},
   "source": [
    "## A Memory to Remember Senses by Location\n",
    "\n",
    "The purpose of the memory is to remember what would be sensed in a given location that was visited. In a continuous setting, we will never visit the exact same location twice, so we do not want a memory _per se_. Instead, we want an interpolator that can predict the expected sensory value well.\n",
    "\n",
    "Suppose, then that our agent has visited a sequence of locations $\\ell_t$, observing $z_t$ at each step, yielding a sequence of pairs $\\{(\\ell_t, z_t)\\}$. Given a new location $\\ell$, we want to estimate $\\hat{z} = f(\\ell)$ provided that for all $t$, $z_t \\approx f(\\ell_t)$. But _this is just a regression_! Our \"memory\" is not really a memory; it is a regression model trained from the dataset of visited points.\n",
    "\n",
    "Our memory, then is a regression function $f$ trained on the visited points. However, we need a model that can be rapidly trained, because the memory needs to be immediately available from timestep to timestep. As a first approach, we can simply interpolate with an attention kernel.\n",
    "\n",
    "In our case, our location estimates $\\ell_t$ are generated by the agent and come with error, which we model as a Gaussian with diagonal covariance matrix (_i.e._, independent variation in each location dimension). Thus to each $\\ell_t \\in \\mathbb{R}^d$ we associate a vector of deviations $\\sigma_t \\in \\mathbb{R}^d$, and we want to regress $\\hat{z} = f(\\ell, \\sigma)$. We can compute a location affinity kernel $k(\\ell, \\ell_t)$ between the inputs $\\ell$ and $\\sigma$ based on the $\\sigma$-scaled distance as \n",
    "$$\n",
    "\\log k(\\ell, \\ell_t) = \\quad-\\frac{1}{2}\\left\\|\\frac{\\ell - \\ell_t}{\\sqrt{\\sigma^2 + \\sigma_t^2}}\\right\\|^2 \n",
    "\\quad-\\sum_i \\log | \\sigma_i^2 + \\sigma_{t,i}^2 | \n",
    "\\quad-\\frac{d}{2}\\log 2\\pi\n",
    "$$\n",
    "where logs make the relationships easier to see. Vector division is componentwise, and $k(\\ell, \\ell_t)$ is just the density function of a Gaussian $\\mathcal{N}\\left(\\ell_t, \\textrm{diag}\\left(\\sigma^2 + \\sigma_t^2\\right)\\right)$ -- the variance combines the measurement error on both $\\ell$ and $\\ell_t$ and represents the variance of $\\ell + \\ell_t$.\n",
    "\n",
    "Next, we can take a softmax over $\\log k$ to get a set of affinity weights $w_t$ that will weight our dataset examples according to their closeness to the query point $\\ell$, accounting for measurement error:\n",
    "$$\n",
    "w_t = \\textrm{softmax} \\left(\\log k(\\ell, \\ell_t)\\right) = \\frac{k(\\ell, \\ell_t)}{\\sum_s k(\\ell, \\ell_s)}\n",
    "$$\n",
    "\n",
    "From here, we can regress directly on the dataset to obtain the sensor estimate $\\hat{z}$ by\n",
    "$$\n",
    "\\hat{z} = \\sum_t w_t z_t,\n",
    "$$\n",
    "which estimates the sensor output as a weighted average of the past sensor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa6401-3dec-4f97-9a57-ffb180938b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def read_memory(self, query_location: torch.Tensor, query_deviation: torch.Tensor, \n",
    "                memory_locations: torch.Tensor, memory_deviation: torch.Tensor, \n",
    "                memory_values: torch.Tensor) -> torch.Tensor:\n",
    "    # we expect query_location to be a tensor of shape (..., num_queries, dim) (but num_queries can be 1 or missing)\n",
    "    # we expect query_deviation to be a tensor of shape (..., num_queries, dim) (but num_queries can be 1 or missing)\n",
    "    # we expect memory_locations to be a tensor of shape (..., num_keys, dim)\n",
    "    # we expect memory_deviation to be a tensor of shape (..., num_keys, dim)\n",
    "    # we expect memory_values to be a tensor of shape (..., num_keys, embed_dim)\n",
    "\n",
    "    single_query = query_location.ndim < memory_locations.ndim\n",
    "    if single_query:\n",
    "        query_locations = query_location[..., None, :]\n",
    "        query_deviations = query_deviation[..., None, :]\n",
    "    \n",
    "    assert query_locations.ndim == query_deviations.ndim == memory_locations.ndim == memory_deviation.ndim == memory_values.ndim\n",
    "\n",
    "    # compute the combined variance, which has shape (..., num_queries, num_keys)\n",
    "    variance = query_deviation**2 + memory_deviation**2\n",
    "    log_k = (\n",
    "        - 0.5 * ((query_location - memory_locations).pow(2) / variance).sum(dim=-1) \n",
    "        - torch.log(variance).sum(dim=-1)\n",
    "        - 0.5 * math.log(2 * math.pi) * variance.shape[-1]\n",
    "    )\n",
    "\n",
    "    # the location affinity weights have shape (..., num_queries, num_keys)\n",
    "    w = torch.softmax(log_k, dim=-1)\n",
    "\n",
    "    hat_z = torch.bmm(w, memory_values)\n",
    "\n",
    "    if single_query:\n",
    "        hat_z = hat_z.squeeze(-2)\n",
    "\n",
    "    return hat_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f9b55",
   "metadata": {},
   "source": [
    "Now, you might notice that this kernel looks very similar to dot product attention, and then you might ask whether we could recast it to make use of efficient tools for handling long-context attention, such as flash attention. The answer is that _you could_, but you would be changing the topology of the location space in so doing, and you would have to work that change all the way through the math. We might do that later. For now, the clarity of keeping our space as $\\mathbb{R}^d$ is preferable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
